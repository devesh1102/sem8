{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SN backpropagation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "52cz_d6IzUW-",
        "colab_type": "code",
        "outputId": "0c4b5c31-1da9-477f-95a3-d2175ff1df3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "! [ ! -z \"$COLAB_GPU\" ] && pip install torch scikit-learn==0.20.* skorch\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import tensorflow\n",
        "# Loading the machine learning packages \n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV \n",
        "from sklearn.metrics import mean_absolute_error, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import preprocessing \n",
        "from sklearn.metrics import roc_curve\n",
        "# from tensorflow.examples.tutorials.mnist import input_data\n",
        "from sklearn import datasets, svm, metrics\n",
        "# from sklearn.datasets import fetch_mldata\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# importing one hot encoder from sklearn \n",
        "from sklearn.preprocessing import OneHotEncoder "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: scikit-learn==0.20.* in /usr/local/lib/python3.6/dist-packages (0.20.4)\n",
            "Requirement already satisfied: skorch in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.4)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.20.*) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (4.41.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.8.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYNp4poboKDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgwBFo_bz0OR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing the Mnist data\n",
        "\n",
        "# mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
        "mnist = fetch_openml('mnist_784', cache=False)\n",
        "\n",
        "\n",
        "# X = X[0:100,:]\n",
        "# y = y[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMnRtCq1z0ES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = mnist.data.astype('float32')\n",
        "y = mnist.target.astype('int64')\n",
        "##selecting a small sample out of 70k to speed up coding and fix bugs\n",
        "samples = 100\n",
        "X = X[0:samples][:]\n",
        "y = y[0:samples]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmiRkR1HpbXY",
        "colab_type": "code",
        "outputId": "9ab3a78e-df28-4245-90ba-97d9aedb9c8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "i = 4\n",
        "plt.imshow(X[i][:].reshape(28,28),cmap='gray')\n",
        "print(y[i])\n",
        "plt.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoY7N3Ncqb3g",
        "colab_type": "code",
        "outputId": "e1dec148-0df9-45d9-eaed-525e2f4e0a97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "y = y.reshape(-1,1)\n",
        "onehotencoder = OneHotEncoder() \n",
        "y_onehot = onehotencoder.fit_transform(y).toarray() \n",
        "eps = 43\n",
        "X = X*eps/np.max(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.25, random_state=42)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS_ppcb9tuC0",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing done X train has training set while X_test has the validation det. Y is in one hot encoding. X's pixels are scaled from 0-1. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1TAqRufsX4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#user defined inputs:\n",
        "t_mp =0.02\n",
        "t_ref = 0.001\n",
        "alpha = 7\n",
        "n_w = 0.003\n",
        "n_th = 0.1*n_w\n",
        "beta = 10\n",
        "l = 0.03\n",
        "rho = 0.0002\n",
        "\n",
        "n_in= 784#must be 28*28\n",
        "n_hidden = 200#variable\n",
        "n_out= 10\n",
        "epochs = 100\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bQfPWm3BafU",
        "colab_type": "text"
      },
      "source": [
        "Here m represents the no of synapse. As the it is a fully connected network nno of synapse in ith layer is equal to no of node in i-1 layer. In input layer hash one synapse only.\n",
        "\n",
        "W[i][j] represent the weight of ith neuron of a layer connected to jth neuron of the next layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qdu1H9q9iLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m_in = 1\n",
        "m_hidden = n_in\n",
        "m_out = n_hidden\n",
        "\n",
        "w_in = [random.uniform(-1*math.sqrt(3/m_in),1*math.sqrt(3/m_in)) for i in range(n_in)]\n",
        "w_hidden = [[random.uniform(-1*math.sqrt(3/m_hidden),1*math.sqrt(3/m_hidden)) for i in range (m_hidden)] for j in range(n_hidden)]\n",
        "w_out = [[random.uniform(-1*math.sqrt(3/m_out),1*math.sqrt(3/m_out)) for i in range(m_out)] for j in range(n_out)]\n",
        "\n",
        "vth_in = [alpha*math.sqrt(3/m_in) for i in range(n_in)]\n",
        "vth_hidden = [alpha*math.sqrt(3/m_hidden) for i in range(n_hidden)]\n",
        "vth_out = [alpha*math.sqrt(3/m_out) for i in range(n_out)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5j-27qHDEDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this fundtion takes in the mean and generates the no of spikes in time ms second.\n",
        "# gives the index value when it spikes. if spikes = [10,20,40]\n",
        "\n",
        "def gen_spike(mean,time):#time is in ms\n",
        "  A = [random.uniform(0,1) for i in range(time)]\n",
        "  threshold = mean/time\n",
        "  spikes = []\n",
        "  count = 0 \n",
        "  for i in range(len(A)):\n",
        "    if A[i] < threshold:\n",
        "      spikes.append(i)\n",
        "      # count= count +1\n",
        "  # print(len(spikes))\n",
        "  return spikes\n",
        "# stimulus =A[A<threshold];\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRF5v9VRZH63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = []\n",
        "img = 1\n",
        "time = 1000#1 s for each image\n",
        "for i in X_train[img]:\n",
        "  inputs.append( gen_spike(i,time))# inputs tell me which neurons recieve spike.inputs[k] has input spike times of kth input neuron\n",
        "spike_time = [ [] for i in range(time+1)]\n",
        "for i in range(len(inputs)):\n",
        "  for j in inputs[i]:\n",
        "    spike_time[j].append(i)# at a perticular time j which all neurons spike is given by spike_time[j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-sAXUn6ZgxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_hidden = [[[] for i in range(m_hidden)] for j in range(n_hidden)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmz9RaDybER4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hid_ispikes = [[] for i in range(time+1)]\n",
        "a_in = [[] for i in range(n_in)]#ouput spike of input layer. a_in[j] has spike time of jth neuron\n",
        "\n",
        "nin_vmp_t=  [[0,0] for i in range(n_in)]# collects infor mation of jth neuron after last inputspike. vmp and time\n",
        "pre_spike = 0# time when last ouputspike occured of this layer\n",
        "for i in range(1,time+1):#1 ms interval \n",
        "  # for i in range(n_in):\n",
        "  next_spike = pre_spike \n",
        "  has_spiked = {}\n",
        "  for j in spike_time[i]:\n",
        "    # if last_v [j] == []:# if jth no input spike before:\n",
        "    # #   v_pre = 0\n",
        "    # #   t_pre = 0\n",
        "    # # else:\n",
        "    # #   v_pre = last_v[j][0]\n",
        "    # #   t_pre = last_v[j][1]\n",
        "    delta_t = (pre_spike - i)/1000\n",
        "    # if delta_t < t_ref :\n",
        "    w_dyn = min((delta_t/t_ref)**2,1)\n",
        "    nin_vmp_t[j][0] = nin_vmp_t[j][0]*math.exp( ( i - nin_vmp_t[j][1]) / (1000*t_mp))  + w_dyn*w_in[j]\n",
        "    nin_vmp_t[j][1] = i\n",
        "    if nin_vmp_t[j][0] > vth_in[j] and j not in has_spiked:\n",
        "      nin_vmp_t[j][0] = 0\n",
        "      next_spike = i\n",
        "      a_in[j].append(i)\n",
        "      hid_ispikes[i].append(j)\n",
        "      has_spiked[j] =1\n",
        "  pre_spike = next_spike\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCDnpsP1TNlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###layer 2 now. it has input spike in hid_spike. hid_ispike[i] has neurons of inputlayer that spiked at ith ms.\n",
        "##a_in[j] has the time when jth neuron spiked\n",
        "a_hidden = [[] for i in range(n_hidden)]#ouput spike of hidden layer\n",
        "out_ispikes = [[] for i in range(time+1)]\n",
        "\n",
        "pre_spike = 0\n",
        "hid_vmp_t=  [[0,0] for i in range(n_hidden)]\n",
        "for i in range(1,time+1):\n",
        "  next_spike = pre_spike\n",
        "  has_spiked = {}\n",
        "  for ispike in hid_ispikes[i]:\n",
        "    for j in range(n_hidden):\n",
        "      #time instant:::::i\n",
        "      #inputspike from ispike\n",
        "      # curr neuron ::::j\n",
        "      delta_t = (pre_spike - i)/1000\n",
        "      w_dyn = min((delta_t/t_ref)**2,1)\n",
        "      hid_vmp_t[j][0] = hid_vmp_t[j][0]*math.exp( ( i - hid_vmp_t[j][1]) / (1000*t_mp))  + w_dyn*w_hidden[j][ispike]\n",
        "      hid_vmp_t[j][1] = i\n",
        "      if hid_vmp_t[j][1]> vth_hidden[j] and j not in has_spiked:\n",
        "        hid_vmp_t[j][0] = 0\n",
        "        next_spike = i\n",
        "        a_hidden[j].append(i)\n",
        "        out_ispikes[i].append(j)\n",
        "        has_spiked[j]= 1\n",
        "  pre_spike = next_spike"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSY4NC9qcjK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###output now. it has input spike in out_ispike. out_ispike[i] has neurons of inputlayer that spiked at ith ms.\n",
        "##a_in[j] has the time when jth neuron spiked\n",
        "a_out = [[] for i in range(n_out)]#ouput spike of ouput layer\n",
        "pre_spike = 0\n",
        "out_vmp_t=  [[0,0] for i in range(n_out)]\n",
        "for i in range(1,time+1):\n",
        "  next_spike = pre_spike\n",
        "  has_spiked = {}\n",
        "  for ispike in out_ispikes[i]:\n",
        "    for j in range(n_out):\n",
        "      #time instant:::::i\n",
        "      #inputspike from ispike\n",
        "      # curr neuron ::::j\n",
        "      delta_t = (pre_spike - i)/1000 \n",
        "      w_dyn = min((delta_t/t_ref)**2,1)\n",
        "      out_vmp_t[j][0] = out_vmp_t[j][0]*math.exp( ( i - out_vmp_t[j][1]) / (1000*t_mp))  + w_dyn*w_out[j][ispike]\n",
        "      out_vmp_t[j][1] = i\n",
        "      if out_vmp_t[j][1]> vth_out[j] and j not in has_spiked  :\n",
        "        out_vmp_t[j][0] = 0\n",
        "        next_spike = i\n",
        "        a_out[j].append(i)\n",
        "        has_spiked[j] =1\n",
        "\n",
        "  pre_spike = next_spike\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVBZyImWK1BR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f82e0ab1-6e3a-4138-bbfa-f949cd3c56d2"
      },
      "source": [
        "# print(spike_time[time-1])\n",
        "print(np.max(spike_time))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[686]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZKldaq4egMx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "bb9d4487-c582-40c9-dd28-11ace1d259b6"
      },
      "source": [
        "# print(hid_ispikes)\n",
        "# print(out_ispikes)\n",
        "print(len(a_out[2]))\n",
        "print(a_out)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "276\n",
            "[[33, 41, 44, 47, 50, 53, 59, 62, 64, 66, 76, 79, 80, 83, 85, 87, 88, 92, 96, 97, 99, 100, 106, 113, 114, 118, 123, 128, 129, 130, 138, 144, 147, 149, 151, 152, 161, 162, 164, 169, 176, 178, 180, 181, 182, 194, 198, 201, 205, 206, 208, 212, 214, 217, 218, 219, 220, 221, 229, 235, 238, 242, 243, 247, 248, 258, 266, 272, 274, 283, 284, 285, 286, 290, 291, 292, 293, 294, 295, 296, 301, 307, 310, 311, 317, 328, 333, 334, 335, 337, 338, 345, 349, 350, 351, 365, 367, 368, 371, 374, 375, 378, 384, 387, 391, 392, 404, 408, 412, 417, 419, 420, 422, 423, 425, 426, 429, 431, 435, 439, 441, 443, 446, 447, 452, 457, 459, 463, 464, 466, 479, 486, 488, 492, 495, 497, 501, 502, 503, 507, 509, 512, 514, 516, 518, 523, 524, 528, 536, 541, 542, 544, 547, 554, 555, 558, 562, 564, 565, 570, 572, 573, 581, 592, 600, 602, 605, 607, 618, 620, 625, 627, 629, 630, 633, 634, 637, 638, 640, 647, 648, 652, 657, 659, 665, 667, 675, 684, 692, 694, 695, 698, 700, 705, 706, 710, 711, 712, 714, 722, 732, 733, 740, 756, 764, 769, 771, 775, 778, 780, 781, 782, 786, 788, 789, 791, 792, 800, 803, 808, 816, 819, 824, 834, 840, 841, 843, 851, 853, 854, 856, 858, 865, 866, 868, 875, 876, 880, 882, 884, 886, 888, 889, 891, 895, 898, 906, 908, 914, 922, 928, 933, 940, 942, 943, 947, 951, 952, 955, 958, 959, 962, 965, 966, 970, 974, 976, 978, 983, 984, 987, 989, 991, 995, 996, 997], [33, 41, 44, 47, 50, 53, 59, 62, 64, 66, 76, 79, 80, 83, 85, 87, 88, 92, 96, 97, 99, 100, 106, 113, 114, 118, 123, 128, 129, 130, 138, 144, 147, 149, 151, 152, 161, 162, 164, 169, 176, 178, 180, 181, 182, 194, 198, 201, 205, 206, 208, 212, 214, 217, 218, 219, 220, 221, 229, 235, 238, 242, 243, 247, 248, 258, 266, 272, 274, 283, 284, 285, 286, 290, 291, 292, 293, 294, 295, 296, 301, 307, 310, 311, 317, 328, 333, 334, 335, 337, 338, 345, 349, 350, 351, 365, 367, 368, 371, 374, 375, 378, 384, 387, 391, 392, 404, 408, 412, 417, 419, 420, 422, 423, 425, 426, 429, 431, 435, 439, 441, 443, 446, 447, 452, 457, 459, 463, 464, 466, 479, 486, 488, 492, 495, 497, 501, 502, 503, 507, 509, 512, 514, 516, 518, 523, 524, 528, 536, 541, 542, 544, 547, 554, 555, 558, 562, 564, 565, 570, 572, 573, 581, 592, 600, 602, 605, 607, 618, 620, 625, 627, 629, 630, 633, 634, 637, 638, 640, 647, 648, 652, 657, 659, 665, 667, 675, 684, 692, 694, 695, 698, 700, 705, 706, 710, 711, 712, 714, 722, 732, 733, 740, 756, 764, 769, 771, 775, 778, 780, 781, 782, 786, 788, 789, 791, 792, 800, 803, 808, 816, 819, 824, 834, 840, 841, 843, 851, 853, 854, 856, 858, 865, 866, 868, 875, 876, 880, 882, 884, 886, 888, 889, 891, 895, 898, 906, 908, 914, 922, 928, 933, 940, 942, 943, 947, 951, 952, 955, 958, 959, 962, 965, 966, 970, 974, 976, 978, 983, 984, 987, 989, 991, 995, 996, 997], [33, 41, 44, 47, 50, 53, 59, 62, 64, 66, 76, 79, 80, 83, 85, 87, 88, 92, 96, 97, 99, 100, 106, 113, 114, 118, 123, 128, 129, 130, 138, 144, 147, 149, 151, 152, 161, 162, 164, 169, 176, 178, 180, 181, 182, 194, 198, 201, 205, 206, 208, 212, 214, 217, 218, 219, 220, 221, 229, 235, 238, 242, 243, 247, 248, 258, 266, 272, 274, 283, 284, 285, 286, 290, 291, 292, 293, 294, 295, 296, 301, 307, 310, 311, 317, 328, 333, 334, 335, 337, 338, 345, 349, 350, 351, 365, 367, 368, 371, 374, 375, 378, 384, 387, 391, 392, 404, 408, 412, 417, 419, 420, 422, 423, 425, 426, 429, 431, 435, 439, 441, 443, 446, 447, 452, 457, 459, 463, 464, 466, 479, 486, 488, 492, 495, 497, 501, 502, 503, 507, 509, 512, 514, 516, 518, 523, 524, 528, 536, 541, 542, 544, 547, 554, 555, 558, 562, 564, 565, 570, 572, 573, 581, 592, 600, 602, 605, 607, 618, 620, 625, 627, 629, 630, 633, 634, 637, 638, 640, 647, 648, 652, 657, 659, 665, 667, 675, 684, 692, 694, 695, 698, 700, 705, 706, 710, 711, 712, 714, 722, 732, 733, 740, 756, 764, 769, 771, 775, 778, 780, 781, 782, 786, 788, 789, 791, 792, 800, 803, 808, 816, 819, 824, 834, 840, 841, 843, 851, 853, 854, 856, 858, 865, 866, 868, 875, 876, 880, 882, 884, 886, 888, 889, 891, 895, 898, 906, 908, 914, 922, 928, 933, 940, 942, 943, 947, 951, 952, 955, 958, 959, 962, 965, 966, 970, 974, 976, 978, 983, 984, 987, 989, 991, 995, 996, 997], [33, 41, 44, 47, 50, 53, 59, 62, 64, 66, 76, 79, 80, 83, 85, 87, 88, 92, 96, 97, 99, 100, 106, 113, 114, 118, 123, 128, 129, 130, 138, 144, 147, 149, 151, 152, 161, 162, 164, 169, 176, 178, 180, 181, 182, 194, 198, 201, 205, 206, 208, 212, 214, 217, 218, 219, 220, 221, 229, 235, 238, 242, 243, 247, 248, 258, 266, 272, 274, 283, 284, 285, 286, 290, 291, 292, 293, 294, 295, 296, 301, 307, 310, 311, 317, 328, 333, 334, 335, 337, 338, 345, 349, 350, 351, 365, 367, 368, 371, 374, 375, 378, 384, 387, 391, 392, 404, 408, 412, 417, 419, 420, 422, 423, 425, 426, 429, 431, 435, 439, 441, 443, 446, 447, 452, 457, 459, 463, 464, 466, 479, 486, 488, 492, 495, 497, 501, 502, 503, 507, 509, 512, 514, 516, 518, 523, 524, 528, 536, 541, 542, 544, 547, 554, 555, 558, 562, 564, 565, 570, 572, 573, 581, 592, 600, 602, 605, 607, 618, 620, 625, 627, 629, 630, 633, 634, 637, 638, 640, 647, 648, 652, 657, 659, 665, 667, 675, 684, 692, 694, 695, 698, 700, 705, 706, 710, 711, 712, 714, 722, 732, 733, 740, 756, 764, 769, 771, 775, 778, 780, 781, 782, 786, 788, 789, 791, 792, 800, 803, 808, 816, 819, 824, 834, 840, 841, 843, 851, 853, 854, 856, 858, 865, 866, 868, 875, 876, 880, 882, 884, 886, 888, 889, 891, 895, 898, 906, 908, 914, 922, 928, 933, 940, 942, 943, 947, 951, 952, 955, 958, 959, 962, 965, 966, 970, 974, 976, 978, 983, 984, 987, 989, 991, 995, 996, 997], [33, 41, 44, 47, 50, 53, 59, 62, 64, 66, 76, 79, 80, 83, 85, 87, 88, 92, 96, 97, 99, 100, 106, 113, 114, 118, 123, 128, 129, 130, 138, 144, 147, 149, 151, 152, 161, 162, 164, 169, 176, 178, 180, 181, 182, 194, 198, 201, 205, 206, 208, 212, 214, 217, 218, 219, 220, 221, 229, 235, 238, 242, 243, 247, 248, 258, 266, 272, 274, 283, 284, 285, 286, 290, 291, 292, 293, 294, 295, 296, 301, 307, 310, 311, 317, 328, 333, 334, 335, 337, 338, 345, 349, 350, 351, 365, 367, 368, 371, 374, 375, 378, 384, 387, 391, 392, 404, 408, 412, 417, 419, 420, 422, 423, 425, 426, 429, 431, 435, 439, 441, 443, 446, 447, 452, 457, 459, 463, 464, 466, 479, 486, 488, 492, 495, 497, 501, 502, 503, 507, 509, 512, 514, 516, 518, 523, 524, 528, 536, 541, 542, 544, 547, 554, 555, 558, 562, 564, 565, 570, 572, 573, 581, 592, 600, 602, 605, 607, 618, 620, 625, 627, 629, 630, 633, 634, 637, 638, 640, 647, 648, 652, 657, 659, 665, 667, 675, 684, 692, 694, 695, 698, 700, 705, 706, 710, 711, 712, 714, 722, 732, 733, 740, 756, 764, 769, 771, 775, 778, 780, 781, 782, 786, 788, 789, 791, 792, 800, 803, 808, 816, 819, 824, 834, 840, 841, 843, 851, 853, 854, 856, 858, 865, 866, 868, 875, 876, 880, 882, 884, 886, 888, 889, 891, 895, 898, 906, 908, 914, 922, 928, 933, 940, 942, 943, 947, 951, 952, 955, 958, 959, 962, 965, 966, 970, 974, 976, 978, 983, 984, 987, 989, 991, 995, 996, 997], [33, 41, 44, 47, 50, 53, 59, 62, 64, 66, 76, 79, 80, 83, 85, 87, 88, 92, 96, 97, 99, 100, 106, 113, 114, 118, 123, 128, 129, 130, 138, 144, 147, 149, 151, 152, 161, 162, 164, 169, 176, 178, 180, 181, 182, 194, 198, 201, 205, 206, 208, 212, 214, 217, 218, 219, 220, 221, 229, 235, 238, 242, 243, 247, 248, 258, 266, 272, 274, 283, 284, 285, 286, 290, 291, 292, 293, 294, 295, 296, 301, 307, 310, 311, 317, 328, 333, 334, 335, 337, 338, 345, 349, 350, 351, 365, 367, 368, 371, 374, 375, 378, 384, 387, 391, 392, 404, 408, 412, 417, 419, 420, 422, 423, 425, 426, 429, 431, 435, 439, 441, 443, 446, 447, 452, 457, 459, 463, 464, 466, 479, 486, 488, 492, 495, 497, 501, 502, 503, 507, 509, 512, 514, 516, 518, 523, 524, 528, 536, 541, 542, 544, 547, 554, 555, 558, 562, 564, 565, 570, 572, 573, 581, 592, 600, 602, 605, 607, 618, 620, 625, 627, 629, 630, 633, 634, 637, 638, 640, 647, 648, 652, 657, 659, 665, 667, 675, 684, 692, 694, 695, 698, 700, 705, 706, 710, 711, 712, 714, 722, 732, 733, 740, 756, 764, 769, 771, 775, 778, 780, 781, 782, 786, 788, 789, 791, 792, 800, 803, 808, 816, 819, 824, 834, 840, 841, 843, 851, 853, 854, 856, 858, 865, 866, 868, 875, 876, 880, 882, 884, 886, 888, 889, 891, 895, 898, 906, 908, 914, 922, 928, 933, 940, 942, 943, 947, 951, 952, 955, 958, 959, 962, 965, 966, 970, 974, 976, 978, 983, 984, 987, 989, 991, 995, 996, 997], [33, 41, 44, 47, 50, 53, 59, 62, 64, 66, 76, 79, 80, 83, 85, 87, 88, 92, 96, 97, 99, 100, 106, 113, 114, 118, 123, 128, 129, 130, 138, 144, 147, 149, 151, 152, 161, 162, 164, 169, 176, 178, 180, 181, 182, 194, 198, 201, 205, 206, 208, 212, 214, 217, 218, 219, 220, 221, 229, 235, 238, 242, 243, 247, 248, 258, 266, 272, 274, 283, 284, 285, 286, 290, 291, 292, 293, 294, 295, 296, 301, 307, 310, 311, 317, 328, 333, 334, 335, 337, 338, 345, 349, 350, 351, 365, 367, 368, 371, 374, 375, 378, 384, 387, 391, 392, 404, 408, 412, 417, 419, 420, 422, 423, 425, 426, 429, 431, 435, 439, 441, 443, 446, 447, 452, 457, 459, 463, 464, 466, 479, 486, 488, 492, 495, 497, 501, 502, 503, 507, 509, 512, 514, 516, 518, 523, 524, 528, 536, 541, 542, 544, 547, 554, 555, 558, 562, 564, 565, 570, 572, 573, 581, 592, 600, 602, 605, 607, 618, 620, 625, 627, 629, 630, 633, 634, 637, 638, 640, 647, 648, 652, 657, 659, 665, 667, 675, 684, 692, 694, 695, 698, 700, 705, 706, 710, 711, 712, 714, 722, 732, 733, 740, 756, 764, 769, 771, 775, 778, 780, 781, 782, 786, 788, 789, 791, 792, 800, 803, 808, 816, 819, 824, 834, 840, 841, 843, 851, 853, 854, 856, 858, 865, 866, 868, 875, 876, 880, 882, 884, 886, 888, 889, 891, 895, 898, 906, 908, 914, 922, 928, 933, 940, 942, 943, 947, 951, 952, 955, 958, 959, 962, 965, 966, 970, 974, 976, 978, 983, 984, 987, 989, 991, 995, 996, 997], [33, 41, 44, 47, 50, 53, 59, 62, 64, 66, 76, 79, 80, 83, 85, 87, 88, 92, 96, 97, 99, 100, 106, 113, 114, 118, 123, 128, 129, 130, 138, 144, 147, 149, 151, 152, 161, 162, 164, 169, 176, 178, 180, 181, 182, 194, 198, 201, 205, 206, 208, 212, 214, 217, 218, 219, 220, 221, 229, 235, 238, 242, 243, 247, 248, 258, 266, 272, 274, 283, 284, 285, 286, 290, 291, 292, 293, 294, 295, 296, 301, 307, 310, 311, 317, 328, 333, 334, 335, 337, 338, 345, 349, 350, 351, 365, 367, 368, 371, 374, 375, 378, 384, 387, 391, 392, 404, 408, 412, 417, 419, 420, 422, 423, 425, 426, 429, 431, 435, 439, 441, 443, 446, 447, 452, 457, 459, 463, 464, 466, 479, 486, 488, 492, 495, 497, 501, 502, 503, 507, 509, 512, 514, 516, 518, 523, 524, 528, 536, 541, 542, 544, 547, 554, 555, 558, 562, 564, 565, 570, 572, 573, 581, 592, 600, 602, 605, 607, 618, 620, 625, 627, 629, 630, 633, 634, 637, 638, 640, 647, 648, 652, 657, 659, 665, 667, 675, 684, 692, 694, 695, 698, 700, 705, 706, 710, 711, 712, 714, 722, 732, 733, 740, 756, 764, 769, 771, 775, 778, 780, 781, 782, 786, 788, 789, 791, 792, 800, 803, 808, 816, 819, 824, 834, 840, 841, 843, 851, 853, 854, 856, 858, 865, 866, 868, 875, 876, 880, 882, 884, 886, 888, 889, 891, 895, 898, 906, 908, 914, 922, 928, 933, 940, 942, 943, 947, 951, 952, 955, 958, 959, 962, 965, 966, 970, 974, 976, 978, 983, 984, 987, 989, 991, 995, 996, 997], [33, 41, 44, 47, 50, 53, 59, 62, 64, 66, 76, 79, 80, 83, 85, 87, 88, 92, 96, 97, 99, 100, 106, 113, 114, 118, 123, 128, 129, 130, 138, 144, 147, 149, 151, 152, 161, 162, 164, 169, 176, 178, 180, 181, 182, 194, 198, 201, 205, 206, 208, 212, 214, 217, 218, 219, 220, 221, 229, 235, 238, 242, 243, 247, 248, 258, 266, 272, 274, 283, 284, 285, 286, 290, 291, 292, 293, 294, 295, 296, 301, 307, 310, 311, 317, 328, 333, 334, 335, 337, 338, 345, 349, 350, 351, 365, 367, 368, 371, 374, 375, 378, 384, 387, 391, 392, 404, 408, 412, 417, 419, 420, 422, 423, 425, 426, 429, 431, 435, 439, 441, 443, 446, 447, 452, 457, 459, 463, 464, 466, 479, 486, 488, 492, 495, 497, 501, 502, 503, 507, 509, 512, 514, 516, 518, 523, 524, 528, 536, 541, 542, 544, 547, 554, 555, 558, 562, 564, 565, 570, 572, 573, 581, 592, 600, 602, 605, 607, 618, 620, 625, 627, 629, 630, 633, 634, 637, 638, 640, 647, 648, 652, 657, 659, 665, 667, 675, 684, 692, 694, 695, 698, 700, 705, 706, 710, 711, 712, 714, 722, 732, 733, 740, 756, 764, 769, 771, 775, 778, 780, 781, 782, 786, 788, 789, 791, 792, 800, 803, 808, 816, 819, 824, 834, 840, 841, 843, 851, 853, 854, 856, 858, 865, 866, 868, 875, 876, 880, 882, 884, 886, 888, 889, 891, 895, 898, 906, 908, 914, 922, 928, 933, 940, 942, 943, 947, 951, 952, 955, 958, 959, 962, 965, 966, 970, 974, 976, 978, 983, 984, 987, 989, 991, 995, 996, 997], [33, 41, 44, 47, 50, 53, 59, 62, 64, 66, 76, 79, 80, 83, 85, 87, 88, 92, 96, 97, 99, 100, 106, 113, 114, 118, 123, 128, 129, 130, 138, 144, 147, 149, 151, 152, 161, 162, 164, 169, 176, 178, 180, 181, 182, 194, 198, 201, 205, 206, 208, 212, 214, 217, 218, 219, 220, 221, 229, 235, 238, 242, 243, 247, 248, 258, 266, 272, 274, 283, 284, 285, 286, 290, 291, 292, 293, 294, 295, 296, 301, 307, 310, 311, 317, 328, 333, 334, 335, 337, 338, 345, 349, 350, 351, 365, 367, 368, 371, 374, 375, 378, 384, 387, 391, 392, 404, 408, 412, 417, 419, 420, 422, 423, 425, 426, 429, 431, 435, 439, 441, 443, 446, 447, 452, 457, 459, 463, 464, 466, 479, 486, 488, 492, 495, 497, 501, 502, 503, 507, 509, 512, 514, 516, 518, 523, 524, 528, 536, 541, 542, 544, 547, 554, 555, 558, 562, 564, 565, 570, 572, 573, 581, 592, 600, 602, 605, 607, 618, 620, 625, 627, 629, 630, 633, 634, 637, 638, 640, 647, 648, 652, 657, 659, 665, 667, 675, 684, 692, 694, 695, 698, 700, 705, 706, 710, 711, 712, 714, 722, 732, 733, 740, 756, 764, 769, 771, 775, 778, 780, 781, 782, 786, 788, 789, 791, 792, 800, 803, 808, 816, 819, 824, 834, 840, 841, 843, 851, 853, 854, 856, 858, 865, 866, 868, 875, 876, 880, 882, 884, 886, 888, 889, 891, 895, 898, 906, 908, 914, 922, 928, 933, 940, 942, 943, 947, 951, 952, 955, 958, 959, 962, 965, 966, 970, 974, 976, 978, 983, 984, 987, 989, 991, 995, 996, 997]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}